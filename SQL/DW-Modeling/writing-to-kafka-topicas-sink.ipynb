{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681d6626-7007-4b01-94bb-19ed29c25a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7710eb7ed668:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming from Kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbf7d1c20e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Streaming from Kafka\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db43dcfb-6132-46e2-8fd8-6b33393b6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "    .option(\"subscribe\", \"device_data\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06009c81-3803-4f70-9be3-481fd62fa880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View schema for raw kafka_df\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "259f852a-e807-45db-b7a8-5c7a0a328118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a3f7c5-9d44-409e-83f8-a9e984569c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse value from binay to string into kafka_json_df\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3159d58d-c13b-4194-a61e-599fb46b4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka_json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4702cfe-5c3c-4dc4-a251-b251cf5b3a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eventId': 'aa90011f-3967-496c-b94b-a0c8de19a3d3',\n",
       " 'eventOffset': 10003,\n",
       " 'eventPublisher': 'device',\n",
       " 'customerId': 'CI00108',\n",
       " 'data': {'devices': [{'deviceId': 'D004',\n",
       "    'temperature': 16,\n",
       "    'measure': 'C',\n",
       "    'status': 'SUCCESS'}]},\n",
       " 'eventTime': '2023-01-05 11:13:53.643364'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Schema of the Pyaload\n",
    "\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, LongType\n",
    "\n",
    "json_schema = (\n",
    "    StructType(\n",
    "    [\n",
    "        StructField('eventId', StringType(), True), \n",
    "        StructField('eventOffset', LongType(), True), \n",
    "        StructField('eventPublisher', StringType(), True),         \n",
    "        StructField('customerId', StringType(), True), \n",
    "        StructField('data', StructType(\n",
    "                                        [StructField('devices', \n",
    "                                             ArrayType(StructType([ \n",
    "                                                StructField('deviceId', StringType(), True), \n",
    "                                                StructField('measure', StringType(), True), \n",
    "                                                StructField('status', StringType(), True), \n",
    "                                                StructField('temperature', LongType(), True)\n",
    "                                            ]), True), True)\n",
    "        ]), True), \n",
    "    StructField('eventTime', StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "{\"eventId\": \"aa90011f-3967-496c-b94b-a0c8de19a3d3\",\n",
    " \"eventOffset\": 10003, \n",
    " \"eventPublisher\": \"device\",\n",
    " \"customerId\": \"CI00108\", \n",
    " \"data\": {\"devices\": [{\"deviceId\": \"D004\", \"temperature\": 16, \"measure\": \"C\", \"status\": \"SUCCESS\"}]},\n",
    " \"eventTime\": \"2023-01-05 11:13:53.643364\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70db2765-9964-43ce-808d-6aa2b4320282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json,col\n",
    "\n",
    "streaming_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema)).selectExpr(\"values_json.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c253f3b0-f441-41de-b39f-77bcae65c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb3194fe-6304-47c9-916f-effec984cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db1279f3-15c9-4edf-9a92-251ed4e201fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explode the data as devices contains list/array of device reading\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "exploded_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0376f739-a5e6-4325-8606-2d8a456c2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema of the exploded_df, place a sample json file and change readStream to read \n",
    "exploded_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afb86297-82b3-4cb0-92c4-a9d5729478a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the exploded df\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "flattened_df = (\n",
    "    exploded_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f369f5aa-c3f5-481b-864c-560e5464e525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema of the flattened_df, place a sample json file and change readStream to read \n",
    "flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efaf7b3a-db3c-4465-9294-58952f57f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31aea3b5-1dd9-406d-bf0c-50a2808eaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json, struct\n",
    "kafka_ready_df  = (\n",
    "    flattened_df\n",
    "    .withColumn(\"value\", to_json(struct(*flattened_df.columns)))  # Convert entire row to JSON\n",
    "    .select(\"value\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6e256-9a16-48aa-a148-a7feeef2453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_kafka = (kafka_ready_df\n",
    "               .writeStream\n",
    "               .format(\"kafka\")\n",
    "               .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "               .option(\"topic\", \"device_data\")  \n",
    "               .option(\"checkpointLocation\", \"checkpoint_dir_kafka\")\n",
    "               .start())\n",
    "\n",
    "query_kafka.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9df2c6e-a786-4431-8699-1ee2ac0ec3f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "Query [id = 26d762a0-c2cd-4dd5-8e51-c84b73bdda77, runId = 474150a0-34e4-4ceb-87c7-04959405399a] terminated with exception: Required attribute 'value' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# console_query = (flattened_df\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#  .writeStream\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  .format(\"console\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#  .start()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#  .awaitTermination())\u001b[39;00m\n\u001b[1;32m     18\u001b[0m (\u001b[43mflattened_df\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43med-kafka:29092\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint_dir_kafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 25\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Query [id = 26d762a0-c2cd-4dd5-8e51-c84b73bdda77, runId = 474150a0-34e4-4ceb-87c7-04959405399a] terminated with exception: Required attribute 'value' not found"
     ]
    }
   ],
   "source": [
    "# console_query = (flattened_df\n",
    "#  .writeStream\n",
    "#  .format(\"console\")\n",
    "#  .outputMode(\"append\")\n",
    "#  .option(\"checkpointLocation\", \"checkpoint_dir_kafka\")\n",
    "#  .start()\n",
    "#  .awaitTermination())\n",
    "\n",
    "# console_query = (flattened_df\n",
    "#  .writeStream\n",
    "#  .format(\"csv\")\n",
    "#  .option(\"path\", \"output_dir\")\n",
    "#  .outputMode(\"append\")\n",
    "#  .option(\"checkpointLocation\", \"checkpoint_dir_kafka\")\n",
    "#  .start()\n",
    "#  .awaitTermination())\n",
    "\n",
    "(flattened_df\n",
    "               .writeStream\n",
    "               .format(\"kafka\")\n",
    "               .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "               .option(\"topic\", \"device_data\")\n",
    "               .option(\"checkpointLocation\", \"checkpoint_dir_kafka\")\n",
    "               .start()\n",
    "              .awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bacd8e-0ca3-49f6-9663-0f3e338ddd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
