{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03827406-4c2c-476d-839c-bbd11d1a6868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-veya9893:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Working with Strings & Dates</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f00c0541390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Working with Strings & Dates\")\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229e2e60-a877-4937-83be-ee08ba2b1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8116b4-c5c9-4707-8d51-063daeabf48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "load_dotenv() \n",
    "\n",
    "api_key = getenv(\"OPENWEATHER_API_KEY\", \"b8f10c8c959340f989701949250801\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224145e8-a479-481c-b770-b082565ec91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b8f10c8c959340f989701949250801'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5fd15c5-9010-4afb-a263-419111aa1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function as a base to get the weather data\n",
    "# This uses the base url and takes the city and endpoint as an argument\n",
    "\n",
    "def get_endpoint_weather_data(city, endpoint, days=None):\n",
    "    base_url = \"http://api.weatherapi.com/v1/\"\n",
    "   \n",
    "    if days:\n",
    "        target_url = f\"{base_url}{endpoint}?key={api_key}&q={city}&days={days}\"\n",
    "    else:\n",
    "        target_url = f\"{base_url}{endpoint}?key={api_key}&q={city}\"\n",
    "\n",
    "    # print(\"Target URL\", target_url)\n",
    "    response = requests.get(target_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data\n",
    "    else:\n",
    "        print(\"Error in GET request\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30e689-8a5a-41a1-8049-6f4f2cc25616",
   "metadata": {},
   "source": [
    "## Get current weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b19ef646-4967-4c5e-b865-a96b8ce03921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_current_weather_data(city=\"Boulder\"):\n",
    "\n",
    "    endpoint = \"current.json\"\n",
    "    data = get_endpoint_weather_data(city, endpoint)\n",
    "\n",
    "    # Extracting data\n",
    "    location = data.get(\"location\").get(\"name\")\n",
    "    region = data.get(\"location\").get(\"region\")\n",
    "    country = data.get(\"location\").get(\"country\")\n",
    "    temp_c = data.get(\"current\").get(\"temp_c\")\n",
    "    temp_f = data.get(\"current\").get(\"temp_f\")\n",
    "\n",
    "    feels_like_c = data.get(\"current\").get(\"feelslike_c\")\n",
    "    feels_like_f = data.get(\"current\").get(\"feelslike_f\")\n",
    "    \n",
    "    local_time = data.get(\"location\").get(\"localtime\")\n",
    "    last_updated_time = data.get(\"current\").get(\"last_updated\")\n",
    "    current_condition = data.get(\"current\").get(\"condition\").get(\"text\")\n",
    "\n",
    "    wind_speed_kph = data.get(\"current\").get(\"wind_kph\")\n",
    "    wind_speed_mph = data.get(\"current\").get(\"wind_mph\")\n",
    "    wind_direction = data.get(\"current\").get(\"wind_dir\")\n",
    "\n",
    "    humidity = data.get(\"current\").get(\"humidity\")\n",
    "\n",
    "    precipitation = data.get(\"current\").get(\"precip_mm\")\n",
    "\n",
    "    uv = data.get(\"current\").get(\"uv\")\n",
    "\n",
    "    # return the extracted data as a dictionary\n",
    "    weather_data = {\n",
    "        \"location\": location,\n",
    "        \"region\": region,\n",
    "        \"country\": country,\n",
    "        \"temp_c\": temp_c,\n",
    "        \"temp_f\": temp_f,\n",
    "        \"feels_like_c\": feels_like_c,\n",
    "        \"feels_like_f\": feels_like_f,\n",
    "        \"local_time\": local_time,\n",
    "        \"last_updated_time\": last_updated_time,\n",
    "        \"current_condition\": current_condition,\n",
    "        \"wind_speed_kph\": wind_speed_kph,\n",
    "        \"wind_speed_mph\": wind_speed_mph,\n",
    "        \"wind_direction\": wind_direction,\n",
    "        \"humidity\": humidity,\n",
    "        \"precipitation\": precipitation,\n",
    "        \"uv\": uv\n",
    "    }\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e1416-3e23-413f-b955-24a1c1c07bfd",
   "metadata": {},
   "source": [
    "## Get Alerts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec552a61-9358-403f-8c28-e3f702e39b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_alerts_data(city=\"Boulder\"):\n",
    "\n",
    "    endpoint = \"alerts.json\"\n",
    "\n",
    "    data = get_endpoint_weather_data(city, endpoint)\n",
    "\n",
    "    # Check if there are any alerts\n",
    "    alerts_exist = data.get(\"alerts\").get(\"alert\")\n",
    " \n",
    "    if len(alerts_exist) == 0:\n",
    "        return {\n",
    "            \"alerts\": \"No alerts for this location\"\n",
    "        }\n",
    "    return data.get(\"alerts\").get(\"alert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac49eb3-a95f-460c-990c-0898475e187e",
   "metadata": {},
   "source": [
    "## Get forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644334d2-4079-4381-8051-9e40f766d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "### Define a function to get the forecast data\n",
    "def get_forecasted_data(days:int, city=\"Boulder\")-> List[dict]:\n",
    "\n",
    "    endpoint = \"forecast.json\"\n",
    "\n",
    "    data = get_endpoint_weather_data(city, endpoint, days=2)\n",
    "\n",
    "    forecasted_data = data.get(\"forecast\").get(\"forecastday\")\n",
    "    \n",
    "    # print(f\"Forecasted data for the next {days} days\", len(forecasted_data))\n",
    "    # Extracting the forecasted data for the next 3 days\n",
    "    # forecasted_data = forecasted_data[:days]\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"date\": day.get(\"date\"),\n",
    "            \"day_condition\": day.get(\"day\").get(\"condition\").get(\"text\"),\n",
    "            \"max_temp_c\": day.get(\"day\").get(\"maxtemp_c\"),\n",
    "            \"min_temp_c\": day.get(\"day\").get(\"mintemp_c\"),\n",
    "            \"max_temp_f\": day.get(\"day\").get(\"maxtemp_f\"),\n",
    "            \"min_temp_f\": day.get(\"day\").get(\"mintemp_f\"),\n",
    "            \"precipitation\": day.get(\"day\").get(\"totalprecip_mm\"),\n",
    "            \"wind_speed_kph\": day.get(\"day\").get(\"maxwind_kph\"),\n",
    "            \"wind_speed_mph\": day.get(\"day\").get(\"maxwind_mph\"),\n",
    "            \"humidity\": day.get(\"day\").get(\"avghumidity\"),\n",
    "            \"snow_cm\": day.get(\"day\").get(\"totalsnow_cm\"),\n",
    "            \"uv\": day.get(\"day\").get(\"uv\"),\n",
    "            \"chances_of_rain\": day.get(\"day\").get(\"daily_chance_of_rain\"),\n",
    "            \"chances_of_snow\": day.get(\"day\").get(\"daily_chance_of_snow\"),\n",
    "        }\n",
    "        for day in forecasted_data\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216092fa-fcac-433a-bf5a-82bee80f27a6",
   "metadata": {},
   "source": [
    "## Flatten the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "528a42b9-fb54-41e0-86a5-acf1b32a00e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Merge the functions into one main function\n",
    "### This will be the flattened dictionary that will be used to store the data\n",
    "\n",
    "def flatten_data():\n",
    "    current_weather_data = get_current_weather_data(city=\"Boulder\")\n",
    "    alerts_data = get_alerts_data(city=\"Boulder\")\n",
    "    forecasted_data = get_forecasted_data(days=3, city=\"Boulder\")\n",
    "\n",
    "    return {\n",
    "        \"current_weather\": current_weather_data,\n",
    "        \"alerts\": alerts_data,\n",
    "        \"forecast\": forecasted_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274787e-d5a6-43fe-afef-8cd232665ee0",
   "metadata": {},
   "source": [
    "## use spark streaming to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7871e13f-0699-4175-9d2f-80c9f2267b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "def process_batch_data(batch_df, batch_id):\n",
    "    try:\n",
    "        print(\"Processing the batch id: {}\".format(batch_id))\n",
    "\n",
    "        # Flatten the data\n",
    "        flattened_data = flatten_data()\n",
    "        from pyspark.sql import Row\n",
    "        rows = [Row(**flattened_data)] \n",
    "        flattened_df = spark.createDataFrame(rows)\n",
    "        \n",
    "     # Write the JSON data to a file\n",
    "        (\n",
    "            flattened_df.write\n",
    "            .mode(\"append\")\n",
    "            .format(\"json\") \n",
    "            .option(\"path\", \"output_dir/json\") \n",
    "            .save()\n",
    "        )\n",
    "\n",
    "        print(f\"Batch {batch_id} written successfully!\")\n",
    "        # Write to console\n",
    "        # flattened_data.show(truncate=False)\n",
    "        \n",
    "        return flattened_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error in processing the batch data\", e)\n",
    "        raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4b3887-eae8-4d2c-bdff-98bdaf5e8574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "streaming_df = (\n",
    "            spark\n",
    "            .readStream\n",
    "            .format(\"rate\")\n",
    "            .option(\"rowsPerSecond\", 1)\n",
    "            .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673c63fd-4d69-4c4d-8898-32e877881203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the batch id: 0\n",
      "Batch 0 written successfully!\n",
      "Processing the batch id: 1\n",
      "Batch 1 written successfully!\n",
      "Processing the batch id: 2\n",
      "Batch 2 written successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[1;32m      2\u001b[0m             \u001b[43mstreaming_df\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_batch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m30 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "(\n",
    "            streaming_df\n",
    "            .writeStream\n",
    "            .foreachBatch(process_batch_data)\n",
    "            .trigger(processingTime=\"30 seconds\")\n",
    "            .outputMode(\"append\")\n",
    "            .start()\n",
    "            .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56434760-010c-4ceb-804d-b0f0359e14f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
